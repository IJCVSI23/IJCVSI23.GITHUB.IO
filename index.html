<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Call for Papers: IJCV Special Issue</title>

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="Call for Papers: IJCV Special Issue"
          .
    >
    <meta name="keywords" content="Call for Papers: IJCV Special Issue">
    <link rel="author" href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">

    <!-- Fonts and stuff -->
    <link href="./css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
    <link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
    <script async="" src="./prettify.js"></script>

</head>

<body>
<div id="content">
    <div id="content-inner">

        <center><img src="./img/IJCV_LOGO_2025-1.png" border="0" width="99.5%"><br>
            <!--            Left: The specification of the proposed 100-Driver for distracted driver classification. Right: The comparisons of different datasets.-->
        </center>

        <div class="section head">
            <h1><a href="https://link.springer.com/journal/11263/updates/27792100">Call for Papers: IJCV Special Issue</a></h1>
            <h1> Ensuring Trustworthiness in Open-World Visual Recognition</h1>

            <!--            <div class="authors">-->
            <!--                <a >Jing Wang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Wenjing Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Fang Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Jun Zhang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Zhongcheng Wu</a><sup>1,2</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Enver Sangineto</a><sup>1</sup>&nbsp;&ndash;&gt;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Stéphane Lathuilière</a><sup>2</sup>&nbsp;&ndash;&gt;-->
            <!--                <a href="https://zhunzhong.site/">Zhun Zhong</a><sup>3</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Moin Nabi</a><sup>3</sup>&nbsp;&ndash;&gt;-->
            <!--                <a >Nicu Sebe</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                1. Chinese Academy of Sciences, China <br>-->
            <!--                2. University of Science and Technology of China, China <br>-->
            <!--                3. University of Trento, Italy-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                wjli007@mail.ustc.edu.cn-->
            <!--            </div>-->
        </div>


        <div class="section abstract">
            <h2>Introduction</h2>
            <br>
            <p>Visual recognition is a fundamental task in computer vision, with applications ranging from autonomous driving to healthcare. However, conventional visual recognition methods are often limited to recognizing a fixed set of classes, which can lead to significant challenges in real-world applications. Therefore, open-world visual recognition has has emerged as a promising solution to address these challenges. However, although recent efforts have focused on improving the robustness of visual recognition systems in open-world setting, they still struggle with several key trustworthiness issues, which include but are not limited to: Unrobustness with Dynamic Environments, Lack of Interpretability, Ethics and Fairness Bias, Privacy and Security Risks, Reliability and Consistency, Insufficient Dynamic Adaptability.</p>
        </div>

        <div class="section abstract">
            <h2>Aims&Scope</h2>
            <br>
            <p>This special issue invites innovative research papers that aim to address these challenges and propose novel techniques for ensuring trustworthiness in open-world visual recognition. Potential topics of interest include, but are not limited to:</p>
            <li><b>Robustness enhancement.</b> This includes methods for improving the robustness of visual recognition systems against adversarial attacks, catastrophic forgetting, domain shifts, and other challenges. </li>
            <li><b>Interpretability and explainability.</b> This includes techniques for making visual recognition systems more interpretable and explainable, enabling users to understand the decision-making process of these systems.</li>
            <li><b>Ethical considerations and fairness.</b> This includes methods for ensuring that visual recognition systems are fair and unbiased, addressing issues related to social bias in training/test data.</li>
            <li><b>Privacy-preserving techniques.</b> This includes methods for protecting sensitive data in visual recog­nition systems, ensuring that user privacy is maintained.</li>
            <li><b>Dynamic adaptability. </b> This includes techniques for enabling visual recognition systems to adapt to new classes and domains in real-time, ensuring that they remain effective in dynamic environments.</li>
            <li><b>Evaluation criteria and benchmarks.</b> This includes the development of new evaluation criteria and benchmarks for assessing the performance of visual recognition systems in open-world scenarios, ensuring that they are robust, interpretable, and fair.</li>
            <p></p>
            <p>Moreover, this special issue also welcomes papers that focus on:</p>
            <li>Developing new techniques for continual learning, federated learning and multi-modality learning in the context of open-world visual recognition</li>
            <li>Exploring the use of large vision-language models or vision foundation models for open-world visual recognition tasks</li>
            <li>Encourage interdisciplinary integration by incorporating perspectives from fields such as ethics, law, and sociology.</li>
            <li>New applications of relational generative models (such as diffusion models) in data enhancement and privacy protection. </li>
            <p></p>
            We encourage submissions that cover a broad range of visual recognition tasks, including but not limited to image classification, object detection, semantic segmentation, action recognition and pose estimation. This special issue will provide a platform for researchers to share their latest findings and contribute to the advancement of trustworthy visual recognition. The contributions in this special is­sue could significantly benefit society by enabling more robust and reliable visual recognition systems, enhancing public safety and security, social impact, and increasing the efficiency of industrial and com­mercial applications. 
        </div>


        <div class="section abstract">
            <h2>Submission Guidelines</h2>
            <br>
            <p>Please submit via IJCV Editorial Manager: <a href="https://www.editorialmanager.com/visi">https://www.editorialmanager.com/visi</a>. Choose S.I: Ensuring Trustworthiness in Open-World Visual Recognition from the Article Type dropdown.</p>
            <p>Submitted papers should present original, unpublished work, relevant to one of the topics of the Special Issue. All submitted papers will be evaluated on the basis of relevance, significance of contribution, technical quality, scholarship, and quality of presentation, by at least three independent reviewers. It is the policy of the journal that no submission, or substantially overlapping submission, be published or be under review at another journal or conference at any time during the review process. Manuscripts will be subject to a peer reviewing process and must conform to the author guidelines available on the IJCV website at: <a href="https://www.springer.com/11263">https://www.springer.com/11263</a>  </p>
        </div>

        <div class="section abstract">
            <h2>Important Dates</h2>
            <br>
            <li> Submission deadline: December 15, 2025 </li>
            <li> First review notification: February 15, 2026</li>
            <li> Revised submission deadline: April 15, 2026</li>
            <li> Final review notification: May 15, 2026</li>
            <li> Final manuscript due: June 15, 2026</li>
            <li>Publication date: Summer 2026</li>
        </div>

        
        <div class="section abstract">
            <h2>Frequently Asked Questions</h2>
            <h4>Q1: Do I need to wait until the first deadline to submit my paper?</h4>
            <p><strong>A:</strong> No. You are welcome to submit your manuscript at any time before the deadline. Once your submission is received, we will initiate the review process as soon as possible.</p>

            <h4>Q2: Are extended versions of conference papers or survey papers eligible for submission?</h4>
            <p><strong>A:</strong> Yes. Both types are welcome. The same criteria apply as for regular IJCV submissions. Extended papers should contain at least 30% new content compared to the original version.</p>

            <h4>Q3: Is there a required template for submissions?</h4>
            <p><strong>A:</strong> There is no mandatory template. As a reference, you may use the following template: <a href="https://www.overleaf.com/read/rjyjmshxmtnx#aa3e95" target="_blank" style="color: #1a73e8;">Overleaf Template</a>.</p>
         </div>

        <div class="section abstract">
            <h2>Senior Guest Editors</h2>
            <br>
            <li>  <a href="https://disi.unitn.it/~sebe">Nicu Sebe</a>: Professor, University of Trento, Italy, <a href = "mailto: niculae.sebe@unitn.it">niculae.sebe@unitn.it</a></li>
                
            <li>  <a href="https://www.comp.nus.edu.sg/cs/people/chuats/">Tat-Seng Chua</a>: National University of Singapore, Singapore, <a href = "mailto: dcscts@nus.edu.sg">dcscts@nus.edu.sg</a></li>

            <li>  <a href="https://www.wjscheirer.com/">Walter Scheirer</a>: University of Notre Dame, USA, <a href = "mailto: walter.scheirer@nd.edu">walter.scheirer@nd.edu</a></li>
                
            <li>  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>: University of Modena and Reggio Emilia, Italy, <a href = "mailto: rita.cucchiara@unimore.it">rita.cucchiara@unimore.it</a></li>

            <li>  <a href="http://faculty.ucmerced.edu/mhyang">Ming-Hsuan Yang</a> : University of California at Merced, USA, <a href = "mailto: mhyang@ucmerced.edu">mhyang@ucmerced.edu</a></li>

        </div>

        <div class="section abstract">
            <h2>Guest Editors</h2>
            <br>

            <li>  <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a>, University of Osaka, Japan and Xiamen University, China <a href = "mailto: hliu@ids.osaka-u.ac.jp">hliu@ids.osaka-u.ac.jp</a> <a href = "mailto: hlynn@xmu.edu.cn">hlynn@xmu.edu.cn</a></li>
                
            <li>  <a href="https://zhunzhong.site">Zhun Zhong</a>: Hefei University of Technology, China, <a href = "mailto: zhunzhong007@gmail.com">zhunzhong007@gmail.com</a></li>

            <li>  <a href="https://jiwei0523.github.io/">Wei Ji</a>: Nanjing University, China <a href = "mailto: weiji@nju.edu.cn">weiji@nju.edu.cn</a></li>

            <li>  <a href="https://zzeng.me/">Zhe Zeng</a>: University of Virginia, USA, <a href = "mailto: zhez@virginia.edu">zhez@virginia.edu</a></li>

        </div>

        <div class="section method">
            <h2>Concact</h2>
            <br>
            If you have any question related to this IJCV SI, please contact <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a> or <a href="https://zhunzhong.site">Zhun Zhong</a> or <a href="https://jiwei0523.github.io/">Wei Ji</a>.
        </div>

</body>
</html>
