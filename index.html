<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Call for Papers: IJCV Special Issue</title>

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="Call for Papers: IJCV Special Issue"
          .
    >
    <meta name="keywords" content="Call for Papers: IJCV Special Issue">
    <link rel="author" href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">

    <!-- Fonts and stuff -->
    <link href="./css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
    <link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
    <script async="" src="./prettify.js"></script>

</head>

<body>
<div id="content">
    <div id="content-inner">

        <center><img src="./img/IJCV_LOGO-1.png" border="0" width="99.5%"><br>
            <!--            Left: The specification of the proposed 100-Driver for distracted driver classification. Right: The comparisons of different datasets.-->
        </center>

        <div class="section head">
            <h1> Call for Papers: IJCV Special Issue</h1>
            <h1> Ensuring Trustworthiness in Open-World Visual Recognition</h1>

            <!--            <div class="authors">-->
            <!--                <a >Jing Wang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Wenjing Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Fang Li</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Jun Zhang</a><sup>1,2</sup>&nbsp;-->
            <!--                <a >Zhongcheng Wu</a><sup>1,2</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Enver Sangineto</a><sup>1</sup>&nbsp;&ndash;&gt;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Stéphane Lathuilière</a><sup>2</sup>&nbsp;&ndash;&gt;-->
            <!--                <a href="https://zhunzhong.site/">Zhun Zhong</a><sup>3</sup>&nbsp;-->
            <!--&lt;!&ndash;                <a href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">Moin Nabi</a><sup>3</sup>&nbsp;&ndash;&gt;-->
            <!--                <a >Nicu Sebe</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                1. Chinese Academy of Sciences, China <br>-->
            <!--                2. University of Science and Technology of China, China <br>-->
            <!--                3. University of Trento, Italy-->
            <!--            </div>-->

            <!--            <div class="affiliations">-->
            <!--                wjli007@mail.ustc.edu.cn-->
            <!--            </div>-->
        </div>


        <div class="section abstract">
            <h2>Introduction</h2>
            <br>
            <p>Visual recognition is a fundamental task in computer vision, with applications ranging from autonomous driving to healthcare. However, conventional visual recognition methods are often limited to recognizing a fixed set of classes, which can lead to significant challenges in real-world applications. Therefore, open-world visual recognition has has emerged as a promising solution to address these challenges. However, although recent efforts have focused on improving the robustness of visual recognition systems in open-world setting, they still struggle with several key trustworthiness issues, which include but are not limited to: Unrobustness with Dynamic Environments: Existing visual recognition methods are often vulnerable to dynamic environments, such as varying lighting conditions, occlusions, and weather changes. These challenges can lead to significant performance degradation in real-world applica­tions, particularly in safety-critical domains like autonomous driving. Lack of Interpretability: Deep models (such as CNNs and transformers) are often considered black boxes, making it difficult to understand their decision-making processes. This lack of inter­pretability can hinder trust in the system, especially in high-stakes applications like healthcare and security. Ethics and Fairness Bias: If there is social bias (such as sample imbalance in gender, race, and region) in the training data, the model may solidify the bias into the recognition results. For example, the recognition accuracy of face recognition systems for people with darker skin tones is significantly lower than that for people with lighter skin tones (a previous study showed that the error gap is more than 10%). Privacy and Security Risks: In scenarios involving faces, medical images, biological features, etc., if the original data is not properly protected, there is a risk of leakage or abuse. For example, if facial data from security cameras is illegally obtained, it may lead to identity theft. On the other hand, some recent studies have shown that attackers can infer the characteristics of training data from model outputs and even reconstruct the original images (such as the abuse of generative adversarial networks), threatening data privacy. Reliability and Consistency: The performance of existing visual recognition methods can be inconsistent across different domains or scenarios. For example, a model trained on synthetic data may not perform well on real-world data. This inconsistency can lead to unreliable results in ap­plications where accuracy is critical, such as autonomous driving and medical diagnosis. Moreover, when fusing sensor data (e.g., images, radar, and infrared), issues like time asynchrony, spatial calibration errors, or data conflicts may result in decision-making errors. Insufficient Dynamic Adaptability: Conventional visual models assume that test data comes from a known set of categories. In the open world, new objects (such as new car models and new species) constantly appear. The model lacks the ability to “recognize the unknown” and may misclassify them as known categories. On the other hand, when the computing power of edge devices (such as drones and mobile phones) is limited, model compression (such as quantization and pruning) may lead to a decrease in robustness and an increase in the misjudgment rate in low-power mode.</p>
        </div>

        <div class="section abstract">
            <h2>Aims&Scope</h2>
            <br>
            <p>This special issue invites innovative research papers that aim to address these challenges and propose novel techniques for ensuring trustworthiness in open-world visual recognition. Potential topics of interest include, but are not limited to:</p>
            <li><b>Robustness enhancement.</b> This includes methods for improving the robustness of visual recognition systems against adversarial attacks, catastrophic forgetting, domain shifts, and other challenges. </li>
            <li><b>Interpretability and explainability.</b> This includes techniques for making visual recognition systems more interpretable and explainable, enabling users to understand the decision-making process of these systems.</li>
            <li><b>Ethical considerations and fairness.</b> This includes methods for ensuring that visual recognition systems are fair and unbiased, addressing issues related to social bias in training/test data.</li>
            <li><b>Privacy-preserving techniques.</b> This includes methods for protecting sensitive data in visual recog­nition systems, ensuring that user privacy is maintained.</li>
            <li><b>Dynamic adaptability. </b> This includes techniques for enabling visual recognition systems to adapt to new classes and domains in real-time, ensuring that they remain effective in dynamic environments.</li>
            <li><b>Evaluation criteria and benchmarks.</b> This includes the development of new evaluation criteria and benchmarks for assessing the performance of visual recognition systems in open-world scenarios, ensuring that they are robust, interpretable, and fair.</li>
            <p></p>
            <p>Moreover, this special issue also welcomes papers that focus on:</p>
            <li>Developing new techniques for continual learning, federated learning and multi-modality learning in the context of open-world visual recognition</li>
            <li>Exploring the use of large vision-language models or vision foundation models for open-world visual recognition tasks</li>
            <li>Encourage interdisciplinary integration by incorporating perspectives from fields such as ethics, law, and sociology.</li>
            <li>New applications of relational generative models (such as diffusion models) in data enhancement and privacy protection. </li>
            <p></p>
            We encourage submissions that cover a broad range of visual recognition tasks, including but not limited to image classification, object detection, semantic segmentation, action recognition and pose estimation. This special issue will provide a platform for researchers to share their latest findings and contribute to the advancement of trustworthy visual recognition. The contributions in this special is­sue could significantly benefit society by enabling more robust and reliable visual recognition systems, enhancing public safety and security, social impact, and increasing the efficiency of industrial and com­mercial applications. 
        </div>


        <div class="section abstract">
            <h2>Submission Guidelines</h2>
            <br>
            <p>Please submit via IJCV Editorial Manager: <a href="https://www.editorialmanager.com/visi">https://www.editorialmanager.com/visi</a>. Choose SI: Open-World Visual Recognition from the Article Type dropdown.</p>
            <p>Submitted papers should present original, unpublished work, relevant to one of the topics of the Special Issue. All submitted papers will be evaluated on the basis of relevance, significance of contribution, technical quality, scholarship, and quality of presentation, by at least three independent reviewers. It is the policy of the journal that no submission, or substantially overlapping submission, be published or be under review at another journal or conference at any time during the review process. Manuscripts will be subject to a peer reviewing process and must conform to the author guidelines available on the IJCV website at: <a href="https://www.springer.com/11263">https://www.springer.com/11263</a>  </p>
        </div>

        <div class="section abstract">
            <h2>Important Dates</h2>
            <br>
            <li> Submission deadline: December 15, 2025 </li>
            <li> First review notification: February 15, 2026</li>
            <li> Revised submission deadline: April 15, 2026</li>
            <li> Final review notification: May 15, 2026</li>
            <li> Final manuscript due: June 15, 2026</li>
            <li>Publication date: Summer 2026</li>
        </div>


        <div class="section abstract">
            <h2>Guest Editors</h2>
            <br>

            <li>  <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a>, The University of Osaka, Japan, <a href = "mailto: hliu@ids.osaka-u.ac.jp">hliu@ids.osaka-u.ac.jp</a></li>
                
            <li>  <a href="https://zhunzhong.site">Zhun Zhong</a>: Hefei University of Technology, China, <a href = "mailto: zhunzhong007@gmail.com">zhunzhong007@gmail.com</a></li>

            <li>  <a href="https://jiwei0523.github.io/">Wei Ji</a>: Nanjing University, China <a href = "mailto: weiji@nju.edu.cn">weiji@nju.edu.cn</a></li>

            <li>  <a href="https://zzeng.me/">Zhe Zeng</a>: University of Virginia, USA, <a href = "mailto: zhez@virginia.edu">zhez@virginia.edu</a></li>
                
            <li>  <a href="https://disi.unitn.it/~sebe">Nicu Sebe</a>: Professor, University of Trento, Italy, <a href = "mailto: niculae.sebe@unitn.it">niculae.sebe@unitn.it</a></li>
                
            <li>  <a href="https://www.comp.nus.edu.sg/cs/people/chuats/">Tat-Seng Chua</a>: National University of Singapore, Singapore, <a href = "mailto: dcscts@nus.edu.sg">dcscts@nus.edu.sg</a></li>

            <li>  <a href="https://www.wjscheirer.com/">Walter Scheirer</a>: University of Notre Dame, USA, <a href = "mailto: walter.scheirer@nd.edu">walter.scheirer@nd.edu</a></li>
                
            <li>  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>: University of Modena and Reggio Emilia, Italy, <a href = "mailto: rita.cucchiara@unimore.it">rita.cucchiara@unimore.it</a></li>

            <li>  <a href="http://faculty.ucmerced.edu/mhyang">Ming-Hsuan Yang</a> : University of California at Merced, USA, <a href = "mailto: mhyang@ucmerced.edu">mhyang@ucmerced.edu</a></li>

        </div>

        <div class="section method">
            <h2>Concact</h2>
            <br>
            If you have any question related to this IJCV SI, please contact <a href="https://lynnhongliu.github.io/hliu">Hong Liu</a> or <a href="https://zhunzhong.site">Zhun Zhong</a> or <a href="https://jiwei0523.github.io/">Wei Ji</a>.
        </div>

</body>
</html>
